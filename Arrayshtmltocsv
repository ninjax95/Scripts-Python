import requests
from bs4 import BeautifulSoup
import csv

# URL de la page contenant les tableaux
url = "URL_DU_SITE_WEB"

# Effectuer une requête HTTP pour récupérer le contenu de la page
response = requests.get(url)

# Vérifier que la requête a réussi
if response.status_code == 200:
    soup = BeautifulSoup(response.text, 'html.parser')

    # Ouvrir un fichier CSV en mode écriture avec l'encodage UTF-8
    with open('donnees.csv', 'w', newline='', encoding='utf-8') as csv_file:
        csv_writer = csv.writer(csv_file, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
        
        # Trouver tous les tableaux sur la page
        tables = soup.find_all('table')

        # Parcourir les tableaux
        for table in tables:
            # Parcourir les lignes du tableau
            for row in table.find_all('tr'):
                # Récupérer toutes les cellules de la ligne
                cells = row.find_all(['td', 'th'])

                # Extraire le texte de chaque cellule en tenant compte des caractères accentués et des apostrophes
                row_data = [cell.get_text(strip=True).replace("\n", "").strip() for cell in cells]

                # Écrire la liste de données dans le fichier CSV
                csv_writer.writerow(row_data)

    print("Les données de tous les tableaux ont été extraites et enregistrées dans 'donnees.csv' avec prise en compte des caractères accentués et des apostrophes.")
else:
    print("Échec de la requête HTTP. Assurez-vous que l'URL est correcte et que le site est accessible.")
